{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IGTI- Desafio Módulo 4",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwANAmLhHkS5"
      },
      "source": [
        "#Instalando o PySpark no Google Colab\n",
        "\n",
        "\n",
        "Instalar o PySpark não é um processo direto como de praxe em Python. Não basta usar um pip install apenas. Na verdade, antes de tudo é necessário instalar dependências como o Java 8, Apache Spark 2.3.2 junto com o Hadoop 2.7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOYuysduHt2v"
      },
      "source": [
        "# instalar as dependências\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvhYi5scHzSM"
      },
      "source": [
        "A próxima etapa é configurar as variáveis de ambiente, pois isso habilita o ambiente do Colab a identificar corretamente onde as dependências estão rodando.\n",
        "\n",
        "Para conseguir “manipular” o terminal e interagir como ele, você pode usar a biblioteca os."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyOdhp0TH1Oh"
      },
      "source": [
        "# configurar as variáveis de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "\n",
        "# tornar o pyspark \"importável\"\n",
        "import findspark\n",
        "findspark.init('spark-2.4.4-bin-hadoop2.7')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPGSsrzdIEcq"
      },
      "source": [
        "Com tudo pronto, vamos rodar uma sessão local para testar se a instalação funcionou corretamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_Mt5SfKIE-8",
        "outputId": "0def11f3-419e-44ad-d981-f323aa90c21b"
      },
      "source": [
        "# iniciar uma sessão local e importar dados do Airbnb\n",
        "from pyspark.sql import SparkSession\n",
        "sc = SparkSession.builder.master('local[*]').getOrCreate()\n",
        "\n",
        "# download do http para arquivo local\n",
        "!wget --quiet --show-progress http://data.insideairbnb.com/brazil/rj/rio-de-janeiro/2019-07-15/visualisations/listings.csv\n",
        "\n",
        "# carregar dados do Airbnb\n",
        "df_spark = sc.read.csv(\"./listings.csv\", inferSchema=True, header=True)\n",
        "\n",
        "# ver algumas informações sobre os tipos de dados de cada coluna\n",
        "df_spark.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "listings.csv        100%[===================>]   4.49M  9.03MB/s    in 0.5s    \n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- host_id: string (nullable = true)\n",
            " |-- host_name: string (nullable = true)\n",
            " |-- neighbourhood_group: string (nullable = true)\n",
            " |-- neighbourhood: string (nullable = true)\n",
            " |-- latitude: string (nullable = true)\n",
            " |-- longitude: string (nullable = true)\n",
            " |-- room_type: string (nullable = true)\n",
            " |-- price: string (nullable = true)\n",
            " |-- minimum_nights: string (nullable = true)\n",
            " |-- number_of_reviews: string (nullable = true)\n",
            " |-- last_review: string (nullable = true)\n",
            " |-- reviews_per_month: string (nullable = true)\n",
            " |-- calculated_host_listings_count: double (nullable = true)\n",
            " |-- availability_365: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9-dwVVOIU7D"
      },
      "source": [
        "Como resultado, você deve ver algo parecido com a imagem abaixo, o que significa que os nossos dados estão rodando corretamente junto ao PySpark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRP1M6-EG-Ec"
      },
      "source": [
        "#Contando palavras Estáticas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IRVqWHxG5nX"
      },
      "source": [
        "#cria a seção a ser utilizada pelo spark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('abc').getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_UMCpP-I0bZ"
      },
      "source": [
        "#lendo o arquivo de texto\n",
        "read_file = spark\\\n",
        ".read\\\n",
        ".format(\"txt\")\\\n",
        ".option(\"inferSchema\", \"true\")\\\n",
        ".text(\"/content/arquivoTexto.txt\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBNfY-mjJg6J",
        "outputId": "a920a802-8c16-48df-ddeb-4bdfbf1911db"
      },
      "source": [
        "#print do esquema do arquivo lindo\n",
        "read_file.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- value: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tP2-HdvlJxCZ",
        "outputId": "cc748a6a-a0d6-4cf0-b792-340157f85de9"
      },
      "source": [
        "#verifica se é um streaming\n",
        "read_file.isStreaming"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZnfDcvIV7ck"
      },
      "source": [
        "#define as bibliotecas  a serem utilizadas\n",
        "from pyspark.sql.functions import explode  #separa os elementos de uma linha em multiplas linhas\n",
        "from pyspark.sql.functions import split    #divide as linhas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO9kGtVlWUwk"
      },
      "source": [
        "#divide as linhas do arquivo\n",
        "words = read_file.select(\n",
        "    explode(\n",
        "        split(read_file.value, \" \")\n",
        "    ).alias(\"word\")\n",
        ")\n",
        "\n",
        "#cria o novo dataframe a ser responsável por contar a quantidade de palavras digitadas\n",
        "wordCounts = words.groupBy(\"word\").count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jfscMdzXKau",
        "outputId": "b08923eb-dd3b-4ae1-e815-9f32840e140c"
      },
      "source": [
        "#realiza o \"plot\" das palavras\n",
        "wordCounts.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----+\n",
            "|      word|count|\n",
            "+----------+-----+\n",
            "|    minhas|    1|\n",
            "|   esqueci|    1|\n",
            "|      vida|    1|\n",
            "|esquecerei|    1|\n",
            "|   caminho|    6|\n",
            "|       tão|    1|\n",
            "|      meio|    6|\n",
            "|    pedra.|    2|\n",
            "|     pedra|    4|\n",
            "|        me|    2|\n",
            "|        de|    1|\n",
            "|     tinha|    7|\n",
            "|     Nunca|    2|\n",
            "|       que|    1|\n",
            "|        No|    1|\n",
            "|   retinas|    1|\n",
            "|        do|    6|\n",
            "|       uma|    7|\n",
            "|   pedrano|    1|\n",
            "|        no|    4|\n",
            "+----------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}